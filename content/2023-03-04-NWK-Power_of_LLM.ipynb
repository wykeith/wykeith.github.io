{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the future of 'chatgpt like'AI models ?\n",
    "\n",
    "![Stable_diffusion_v2_generated][chatbots]\n",
    "\n",
    "* Chatgpt is the latest popular tech buzz around the block\n",
    "\n",
    "* It was published in singapore's [mainstream paper](https://www.straitstimes.com/tech/civil-servants-to-soon-use-chatgpt-to-help-with-research-speech-writing) that all civil servants will be using chatgpt for writing reports & speeches \n",
    "\n",
    "* It was also mentioned in [parliament](https://www.moe.gov.sg/news/parliamentary-replies/20230206-artificial-intelligence-technologies-chatgpt) by Education Minister Chan Chun Sing quote \"... artificial intelligence (AI) tool ChatGPT could be a useful tool for learning ... when students have mastered basic concepts and thinking skills\" unquote\n",
    "\n",
    "Chatgpt in essence is a Large Language Model (LLM) that is built for a dialogue setting (hence the prefix chat) by using reinforcement learning techniques with human feedback loops. In simple english it is self-taught to learn natural langauge with human feedback\n",
    "\n",
    "Its parameters (or the number of internal representational variables in layman terms) are massive at 175 billion, hence the label Large Language Model.\n",
    "\n",
    "LLMs are often cited as foundational models because they are the basic building block for various Natural Language Understanding applications, of which I'm evaluating and exploiting for this post.\n",
    "\n",
    "Currently due to the massiveness of the model, it is almost difficult or impossible for users to run such models on consumer-grade desktops or laptops. These models require specialized AI hardware accelerators (GPUs or TPUs) in order be to performant. The bigger models will require larger memory specs in order to keep track of the billions of parameters.\n",
    "\n",
    "I'm grateful and fortunate that I undertook training under AI Singapore's AI apprenticeship programme to be exposed to developing and operating these complex AI models for real-world applications at scale. Allowing me to put together this basic demonstration of the potential of LLM as a useful toolkit.\n",
    "\n",
    "[chatbots]: https://wykeith.github.io/images/google-t5-b.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM model type:\n",
      "\t {'model_name': 'google/flan-t5-xxl', 'model_path': 'google/flan-t5-xxl'}\n",
      "LLM model arguments:\n",
      "\t ['temperature', 'max_tokens', 'n', 'top_p', 'top_k', 'repetition_penalty', 'do_sample', 'client_timeout']\n"
     ]
    }
   ],
   "source": [
    "# Setup + configs for lambdacloud instance\n",
    "\n",
    "lambdacloud = backend(\n",
    "    client_name = \"huggingface\",\n",
    "    client_connection = \"http://localhost:5000\"\n",
    ")\n",
    "print(\"LLM model type:\\n\\t\",lambdacloud.client.get_model_params())\n",
    "print(\"LLM model arguments:\\n\\t\",lambdacloud.client.get_model_inputs())\n",
    "\n",
    "llm = create_model(client=lambdacloud, llm_kwargs={\"temperature\": 0.2, \"max_tokens\": 4096})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSLE passage extracted from \n",
    "# https://static1.squarespace.com/static/5a8433dffe54efed2118456f/t/60ab98e365ffd53bbdce87af/1621858531970/PSLE+English+OE+Comprehension+-+Questions+Only.pdf\n",
    "\n",
    "PSLE_passage = \"\"\"\n",
    "        “Jinhang! How many times have I told you not to use the computer\n",
    "        for so long? Do you want thicker lenses for your glasses?” Mother’s voice\n",
    "        shot out from the kitchen.\n",
    "        I got out of my seat and walked into the kitchen. Mother was\n",
    "        making dumplings.\n",
    "        I sat down beside her and picked up some dough. Seeing this, Mother\n",
    "        smiled at me, “Yes, that’s a good girl.”\n",
    "        I started wrapping, finishing each dumpling off with a clumsy pinch.\n",
    "        After I had made ten dumplings, I looked at my handiwork, dissatisfied.\n",
    "        Picking up my tray of dumplings, I walked towards the wastepaper basket.\n",
    "        Mother stopped me, saying, “They don’t look like much, but they will be\n",
    "        tasty nevertheless.” I sat back down, then tried to continue making more\n",
    "        dumplings, but I found myself unconsciously looking at the ugly ones I had\n",
    "        made earlier.\n",
    "        “Do you remember when you were younger? You were always sick,”\n",
    "        Mother started. “At school, the other kids kicked you, and snatched things\n",
    "        from you, but you never fought back.” I winced at the memory.\n",
    "        Growing up, I was a timid child. Once I had taken my favourite toy\n",
    "        lion to kindergarten. Everyone swarmed around me, admiring it. I loved\n",
    "        the attention. That afternoon, my classmate grabbed the lion, claiming it\n",
    "        as hers. When I finally summoned the courage to tell Mother about this, I\n",
    "        expected her to comfort me. Instead, she gave me one of the worst\n",
    "        telling-offs of my life.\n",
    "        As if reading my thoughts, Mother continued, “Do you know why I\n",
    "        reprimanded you? You did nothing wrong. Father and I knew you were\n",
    "        vulnerable, and needed you to learn to defend yourself.” Then it hit me.\n",
    "        All these years, I thought I had been scolded because I was useless and\n",
    "        unable to protect my belongings, but all Mother wanted was to teach me\n",
    "        to stand up for myself.\n",
    "        “Now look carefully at this big ball of dough,” Mother said, pointing\n",
    "        to the bowl. “That used to be me. Then I decided to sacrifice part of me\n",
    "        for you.” She plucked off a small piece of dough and flattened it with a\n",
    "        rolling pin.\n",
    "        “Now examine this wrapper.” She held up the thin sheet of dough.\n",
    "        “This was you when you were born. At that time even a squeeze could\n",
    "        scar you for life.” Mother loaded a tablespoon of filling into the centre of\n",
    "        the wrapper. “Then we started added in filling: carrots, vegetables, and\n",
    "        mushrooms. We taught you values and equipped you with all the life skills\n",
    "        you needed.”\n",
    "        “As a last move, we sealed our gift from God - with love.” Mother\n",
    "        pinched the dumpling close. I smiled, knowing that I now had to be\n",
    "        independent.\n",
    "        \"\"\"\n",
    "with open(\"psle.txt\",\"w\") as f:\n",
    "    f.write(PSLE_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jinhang was a timid child. When she was scolded by her mother, she thought she was useless and unable to protect her belongings. But all these years, Mother wanted to teach her to stand up for herself.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarization task\n",
    "\n",
    "text_splitter = tokenizer()\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "summarizer = reduceinputs.from_params(llm, prompt, text_splitter)\n",
    "summarizer.run(PSLE_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Given the following document, provide a final answer. \n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"summaries\", \"question\"])\n",
    "answerqns = qainputs(llm, chain_type=\"stuff\", prompt=prompt)\n",
    "docs = readdoc(\"psle.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'Do you want thicker lenses for your glasses?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QA Task\n",
    "query = \"Which two phrase in the first paragraph suggests that the writer did not have good eyesight\"\n",
    "answerqns({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'Mother smiled at the writer and said, “Yes, that’s a good girl.”'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QA Task\n",
    "query = \"List two actions which showed that the writer’s mother approved of the writer helping out with the dumplings in the kitchen.\"\n",
    "answerqns({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'True'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification Task\n",
    "query = \"Mother agreed that the dumplings the writer made did not look beautiful. True or False?\"\n",
    "answerqns({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'False'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification Task\n",
    "query = \"The writer was bullied in school because she was always sick. True or False?\"\n",
    "answerqns({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary thoughts\n",
    "1. As demonstrated above, an open-sourced pre-trained google t5 LLM (11B parameters) can effectively provide answers to comprehension exam questions (abet at PSLE for now), using customized prompting templates.\n",
    "2. Such LLM can be quickly reprogrammed to perform different task using various prompts. Given this flexibility, the possibilities for the developers are unlimited.\n",
    "3. What I will like to build is a personalized assistant that can read through all emails and provide summarization assistance. Using a more intuitive dialogue UI, LLM assistants can scan private emails and provide concise answers to directed queries, all in the safety of a trusted compute environment.\n",
    "4. From a knowledge management/documentation perspective, it can also help officers improve productivity especially during handovers, especially when incumbent officers need to get up to speed on the details of pertinent terms and conditions regarding the history of the project.\n",
    "5. As shared in my [previous post](http://keith.ml/lognmetrics.html), in my past life I used to spend at least 2 hours everyday reading and writing emails, these 2 hours can be effectively cut down by half to 1 hour instead.\n",
    "6. Excited that Meta Research has offered interested parties access to their 65B [LLAMA](https://github.com/facebookresearch/llama) model. Hopefully that will be for another post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': 'The future of large language models is to be used in a variety of applications, including speech recognition, speech synthesis, and natural language processing.',\n",
       "   'generation_info': None}],\n",
       " [{'text': 'no', 'generation_info': None}]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lastly why not lets ask the model about itself\n",
    "llm.generate([\"What is the future of large language models?\",\"Will they replace humans?\"]).to_dict()['generations']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "18311e8a14e4c5a7fb7e4e8140527e030e7835421538e087fc8c73a84d8d7e44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
